import os
import shutil
import pandas as pd # On utilise Pandas pour sauver le fichier sans bugger
from pyspark.sql import SparkSession

# 1. Config Spark ultra-lÃ©gÃ¨re
spark = SparkSession.builder \
    .appName("AutoStreamSilver") \
    .getOrCreate()

spark.sparkContext.setLogLevel("ERROR")
print("ğŸš€ Spark est prÃªt pour le traitement...")

def run_silver_layer():
    # Chemins
    bronze_path = os.path.join(os.getcwd(), "data", "bronze", "json")
    silver_path = os.path.join(os.getcwd(), "data", "silver", "telemetry")

    print("--- ğŸ“¥ PHASE 1 : Ingestion Spark ---")
    try:
        # On lit les JSON avec Spark
        df_spark = spark.read.json(f"data/bronze/json/*.json")
        print(f"âœ… DonnÃ©es lues par Spark : {df_spark.count()} lignes")
        
        # --- ğŸ§¹ PHASE 2 : Nettoyage Spark ---
        # On fait le boulot de Data Engineer avec Spark
        df_clean = df_spark.dropna(subset=["vin"]).fillna({'temp_moteur': 90.0, 'vitesse': 0})
        
        print("--- ğŸ’¾ PHASE 3 : Sauvegarde (Bypass Hadoop) ---")
        
        # Ã‰TAPE MAGIQUE : On transforme en Pandas pour Ã©viter l'erreur de DLL Hadoop
        # Pour 10 lignes ou 1 million de lignes, Ã§a passera sur ton PC
        df_pandas = df_clean.toPandas()
        
        # On crÃ©e le dossier proprement avec Python
        if os.path.exists(silver_path):
            shutil.rmtree(silver_path)
        os.makedirs(silver_path, exist_ok=True)
        
        # On sauvegarde en CSV (plus simple pour le TP et lisible partout)
        output_file = os.path.join(silver_path, "telemetry_clean.csv")
        df_pandas.to_csv(output_file, index=False)
        
        print(f"âœ¨ ZONE SILVER TERMINÃ‰E !")
        print(f"ğŸ“ Fichier crÃ©Ã© ici : {output_file}")
        print(df_pandas.head())

    except Exception as e:
        print(f"âŒ Erreur : {e}")

if __name__ == "__main__":
    run_silver_layer()
    spark.stop()